% !TeX spellcheck = en_US

%
% This is a borrowed LaTeX template file for lecture notes for CS267,
% Applications of Parallel Computing, UCBerkeley EECS Department.
% Now being used for CMU's 10725 Fall 2012 Optimization course
% taught by Geoff Gordon and Ryan Tibshirani.  When preparing 
% LaTeX notes for this class, please use this template.
%
% To familiarize yourself with this template, the body contains
% some examples of its use.  Look them over.  Then you can
% run LaTeX on this file.  After you have LaTeXed this file then
% you can look over the result either by printing it out with
% dvips or using xdvi. "pdflatex template.tex" should also work.
%

\documentclass[twoside]{article}
\setlength{\oddsidemargin}{0.25 in}
\setlength{\evensidemargin}{-0.25 in}
\setlength{\topmargin}{-0.6 in}
\setlength{\textwidth}{6.5 in}
\setlength{\textheight}{8.5 in}
\setlength{\headsep}{0.75 in}
\setlength{\parindent}{0 in}
\setlength{\parskip}{0.1 in}

%
% ADD PACKAGES here:
%
\usepackage{graphicx}
\usepackage{amsmath,amsfonts,graphicx}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{lipsum}
 \usepackage[margin=1in]{geometry} 
\usepackage{amsthm,amssymb}
\usepackage{bm}

\usepackage{booktabs}
%\renewcommand{\familydefault}{pag}
%\renewcommand{\familydefault}{pbk}
% https://en.wikibooks.org/wiki/LaTeX/Fonts

%
% The following commands set up the lecnum (lecture number)
% counter and make various numbering schemes work relative
% to the lecture number.
%
\newcounter{lecnum}
\renewcommand{\thepage}{\thelecnum-\arabic{page}}
\renewcommand{\thesection}{\thelecnum.\arabic{section}}
\renewcommand{\theequation}{\thelecnum.\arabic{equation}}
\renewcommand{\thefigure}{\thelecnum.\arabic{figure}}
\renewcommand{\thetable}{\thelecnum.\arabic{table}}

%
% The following macro is used to generate the header.
%
\newcommand{\lecture}[7]{
   \pagestyle{myheadings}
   \thispagestyle{plain}
   \newpage
   \setcounter{lecnum}{#1}
   \setcounter{page}{1}
   \noindent
   \begin{center}
   \framebox{
      \vbox{\vspace{2mm}
    \hbox to 6.28in { {\bf Machine Learning
	\hfill Spring 2017} }
       \vspace{4mm}
       \hbox to 6.28in { {\Large \hfill Solution : #2  \hfill} }
       \vspace{2mm}
       \hbox to 6.28in { {\it Lecturer: #3 \hfill Homework taker: #4} }
      \vspace{2mm}}
   }
   \end{center}
   \markboth{Solution : #2}{Solution : #2}
}

\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}

\newenvironment{problem}[2][Problem]{\begin{trivlist}
		\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}

%
% Convention for citations is authors' initials followed by the year.
% For example, to cite a paper by Leighton and Maggs you would type
% \cite{LM89}, and to cite a paper by Strassen you would type \cite{S69}.
% (To avoid bibliography problems, for now we redefine the \cite command.)
% Also commands that create a suitable format for the reference list.
\renewcommand{\cite}[1]{[#1]}
\def\beginrefs{\begin{list}%
        {[\arabic{equation}]}{\usecounter{equation}
         \setlength{\leftmargin}{2.0truecm}\setlength{\labelsep}{0.4truecm}%
         \setlength{\labelwidth}{1.6truecm}}}
\def\endrefs{\end{list}}
\def\bibentry#1{\item[\hbox{[#1]}]}

%Use this command for a figure; it puts a figure in wherever you want it.
%usage: \fig{NUMBER}{SPACE-IN-INCHES}{CAPTION}
\newcommand{\fig}[3]{
			\vspace{#2}
			\begin{center}
			Figure \thelecnum.#1:~#3
			\end{center}
	}
% Use these for theorems, lemmas, proofs, etc.
\newtheorem{theorem}{Theorem}[lecnum]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{remark}[theorem]{Remark}
%\newenvironment{proof}{{\bf Proof:}}{\hfill\rule{2mm}{2mm}}
\newenvironment{solution}{{\bf Solution:}}{\hfill\rule{2mm}{2mm}}

% **** IF YOU WANT TO DEFINE ADDITIONAL MACROS FOR YOURSELF, PUT THEM HERE:

\newcommand\E{\mathbb{E}}

\begin{document}
%FILL IN THE RIGHT INFO.
%\lecture{**LECTURE-NUMBER**}{**DATE**}{**LECTURER**}{**SCRIBE**}
\lecture{2}{Homework 2}{Yang Yang}{Li Xu}



 % \textbf{Notification:} You can take 5 problems randomly from all of 15 problems except the problem you design.
  \textbf{Due Time:}April 9

\begin{problem}{1}
\end{problem}

\begin{solution}
	\begin{align*}
	\arg \min_f \mathbb{E}\ell_{\alpha,\beta}(f(x),&y) = \arg \min_f \mathbb{E}_{X,Y}[\alpha\mathbf{1}\{f(X)=1,Y=0\}+\beta \mathbf{1}\{f(X)=0,Y=1\}] \\ 
	&= \arg \min_f \mathbb{E}_X[\mathbb{E}_{Y|X}[\alpha\mathbf{1}\{f(X)=1,Y=0\}+\beta\mathbf{1}\{f(X)=0,Y=1\}]] \\
	&= \arg \min_f \mathbb{E}_X[\int_y \alpha\mathbf{1}\{f(X)=1,y=0\}+\beta\mathbf{1}\{f(X)=0,y=1\}dP(y|x)] \\
	&=\arg\min_f\int_x[\alpha\mathbf{1}\{f(x)=1\}P(y=0|x)+\beta\mathbf{1}\{f(x)=0\}P(y=1|x)]dP(x)
	\end{align*}
	
	Thus, we can minimize the integrand by taking:
	\begin{equation*}
	\left.f(x)=\lbrace
	\right.
	\begin{aligned}
	1 \quad \beta P(y=1|x) \ge \alpha P(y=0|x) \\
	0 \quad \alpha P(y=0|x) > \beta P(y=1|x)
	\end{aligned}
	\end{equation*}

\end{solution}

\begin{problem}{2}
\end{problem} 
\begin{solution}
	
\emph{Answer for problem (1):}

Likelihood term:
\begin{align*}
P(x_1,\cdots,x_N|\mu) &= \prod_{i=1}^{N}P(x_i|\mu) \\
&=\prod_{i=1}^{N}\frac{1}{\sqrt{2\pi \sigma^2}}e^{-\frac{(x_i-\mu)^2}{2\sigma^2}}
\end{align*}

$$
\log(P(x_1,\cdots,x_N|\mu)) = \sum_{i=1}^{N}-\frac{(x_i-\mu)^2}{2\sigma^2}\log(\frac{1}{\sqrt{2\pi \sigma^2}})
$$

$$
\frac{d\log(P(x_1,\cdots,x_N|\mu))}{d\mu} = \sum_{i=1}^{N}\frac{x_i-\mu}{\sigma^2}
$$

If the left part of equation equals to 0:
\begin{align*}
&\sum_{i=1}^{N}\frac{x_i-\mu}{\sigma^2} = 0 \\
&\sum_{i=1}^{N}(x_i-\mu) = 0 \\
&\sum_{i=1}^{N}\mu = \sum_{i=1}^{N}x_i 
\end{align*}

MLE estimator for the mean $\mu$:
$$
\hat{\mu} = \frac{\sum_{i=1}^{N}x_i}{N}
$$

\emph{Answer for problem (2):}
$$
P(\mu|x_1,\cdots,x_N) = \frac{P(x_1,\cdots,x_N|\mu)P(\mu)}{P(x_1,\cdots,x_N)}
$$
where
$$
P(\mu) = \frac{1}{\sqrt{2\pi\beta^2}}e^{-\frac{(x_i-\mu)^2}{2\sigma^2}}
$$
The problem is reduce to find the value of $\mu$ which maximizes:
$$
P(\mu|x_1,\cdots,x_N) = \frac{(\prod_{i=1}^{N}\frac{1}{\sqrt{2\pi \sigma^2}}e^{-\frac{(x_i-\mu)^2}{2\sigma^2}})\frac{1}{\sqrt{2\pi\beta^2}}e^{-\frac{(\mu-\upsilon)^2}{2\beta^2}}}{P(x_1,\cdots,x_N)}
$$

$$
\log P(\mu|x_1,\cdots,x_N) = (\sum_{i=1}^{N}\frac{(x_i-\mu)^2}{2\sigma^2}\log(\sqrt{2\pi\sigma^2})+\frac{(\mu-\upsilon)^2}{2\beta^2}\log(\sqrt{2\pi\beta^2}) 
$$

$$
\frac{\partial\log P(\mu|x_1,\cdots,x_N)}{\partial\mu}=(\sum_{i=1}^{N}\frac{x_i-\mu}{\sigma^2})-\frac{\mu-\upsilon}{\beta^2}
$$
If the left part of equation equals to 0:
\begin{align*}
(\sum_{i=1}^{N}\frac{x_i-\mu}{\sigma^2})-\frac{\mu-\upsilon}{\beta^2} &= 0 \\
(\sum_{i=1}^{N}\frac{x_i-\mu}{\sigma^2}) &=\frac{\mu-\upsilon}{\beta^2} \\
\frac{\mu}{\beta^2}+\frac{N\mu}{\sigma^2} &= \frac{\sum_{i=1}^{N}x_i}{\sigma^2}+\frac{\upsilon}{\beta^2}  \\
\frac{(\sigma^2+N\beta^2)\mu}{\sigma^2\beta^2} &=\frac{\sigma^2\upsilon+\beta^2\sum_{i=1}^{N}x_i}{\sigma^2\beta^2}
\end{align*}

MLE estimator for the mean $\mu$:
$$
\hat{\mu}=\frac{\sigma^2\upsilon+\beta^2\sum_{i=1}^{N}x_i}{\sigma^2+N\beta^2}
$$

\emph{Answer for problem (3):}
\begin{align*}
N &\rightarrow \infty \\
\frac{\sigma^2}{N\beta^2} &\rightarrow 0 \\
\frac{\sigma\upsilon}{N\beta^2+\sigma^2} &\rightarrow 0 \\
\hat{\mu}_MAP &\rightarrow \frac{\sum_{i=1}^{N}x_i}{N} = \hat{\mu}_MLE
\end{align*}
\end{solution}

\begin{problem}{3}
\end{problem}

\begin{solution}

\emph{Answer for problem (1):}
$$P(Class = X) = \frac{2}{3}, P(Class = Y) = \frac{1}{3}$$
$$
P(A1=0|Class = X) = \frac{1}{2},P(A1=1|Class = X) = \frac{1}{4},P(A1=2|Class = X) = \frac{1}{4}
$$
$$
P(A2=0|Class = X) = 0,P(A2=1|Class = X) = \frac{3}{4},P(A2=2|Class = X) = \frac{1}{4}
$$
$$
P(A1=0|Class = Y) = 0,P(A1=1|Class = Y) = \frac{1}{2},P(A1=2|Class =Y) = \frac{1}{2}
$$
$$
P(A2=0|Class = Y) = \frac{1}{2},P(A2=1|Class = Y) = 0,P(A2=2|Class =Y) = \frac{1}{2}
$$
$$
P(Class=X|A1=2,A2=2)=P(Class=X)P(A1=2|Class=X)P(A2=2|Class=X)=\frac{1}{24}
$$
$$
P(Class=Y|A1=2,A2=2)=P(Class=Y)P(A1=2|Class=Y)P(A2=2|Class=Y)=\frac{1}{12}
$$
So Class is predicted to Y.

\emph{Answer for problem (2):}
$$P(Class = X) = \frac{5}{8}, P(Class = Y) = \frac{3}{8}$$
$$
P(A1=0|Class = X) = \frac{3}{7},P(A1=1|Class = X) = \frac{2}{7},P(A1=2|Class = X) = \frac{2}{7}
$$
$$
P(A2=0|Class = X) = \frac{1}{7},P(A2=1|Class = X) = \frac{4}{7},P(A2=2|Class = X) = \frac{2}{7}
$$
$$
P(A1=0|Class = Y) = \frac{1}{5},P(A1=1|Class = Y) = \frac{2}{5},P(A1=2|Class =Y) = \frac{2}{5}
$$
$$
P(A2=0|Class = Y) = \frac{2}{5},P(A2=1|Class = Y) = \frac{1}{5},P(A2=2|Class =Y) = \frac{2}{5}
$$
$$
P(Class=X|A1=2,A2=2)=P(Class=X)P(A1=2|Class=X)P(A2=2|Class=X)=\frac{5}{98}
$$
$$
P(Class=Y|A1=2,A2=2)=P(Class=Y)P(A1=2|Class=Y)P(A2=2|Class=Y)=\frac{3}{50}
$$
Result of previous question do not change.
\end{solution}

\begin{problem}{4}
\end{problem} 
\begin{solution}
	
	The classification accuracy is 83.33\%
	
	Using col 7 - col 54 the accuracy raise to 85.00\%
\end{solution}

\end{document}